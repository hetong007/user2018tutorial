XGBoost
========================================================
author: Tong He 
date: July 11th, 2018
autosize: true

Outline
========================================================

- Introduction
- A tree-based model
- Training
- Interpretation
- Parameter-Tuning

Introduction
========================================================

Simply 

```{r, eval=FALSE}
install.packages('xgboost')
```

Load in

```{r}
library('xgboost')
```

eXtreme Gradient Boosting
========================================================

Base Model + Boosting

Tree-based Model
========================================================

![](./img/cart.png)

Tree-based Model
========================================================

[Explanation in Animation](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)

Why Tree-based Model
========================================================

Advantages

- Interpretable
- Efficient
- Accurate
- No need to normalize

Boosting
========================================================

What to do with a weak model?

- Make it stronger!
- **Ask for help from the others**

Boosting
========================================================

Go beyond a single tree

![](./img/twocart.png)

Boosting
========================================================

- Add simultaneously
  - random forest
- Add **in sequential**

Boosting
========================================================
incremental: true

Iterative algorithm

- Iter 1
  - predict $y$ with $f_1(x)$
  - calculate $r_1 = y - f_1(x)$
- Iter 2
  - predict $r_1$ with $f_2(x)$
  - calculate $r_2 = r_1 - f_2(x)$
- ...


Every new $f_i(x)$ improves from $f_{i-1}(x)$

Boosting
========================================================

One step further

- replace $r_1$ with $L(y, f_1(x))$
- ...
- replace $r_T$ with $L(y, \sum_t^T f_t(x))$

$$
\begin{align}
L[y, \sum_t^T f_t(x)] & = L[y, \sum_t^{T-1} f_t(x) + f_T(x)] \\
& \approx L[y, \sum_t^{T-1} f_t(x)] + g(x)f_T(x)
\end{align}
$$

Boosting
========================================================

Model

$$ pred = \sum_{t=1}^{T} f_t(x) $$

Objective

$$ Obj =  L[\sum_{t=1}^{T} f_t(x), y] $$


Why XGBoost
========================================================

- Regularization

$$ Obj = \sum_{t=1}^{T} L(f_t(x), y) + \Omega(f_t)$$

- Using both first and second order gradient

$$L(f_t(x), y) \approx g(x)f_t(x) + h(x)f^2_t(x)$$

- Prune on a full binary tree

Training
========================================================

[Human Resource Analytics](https://github.com/ryankarlos/Human-Resource-Analytics-Kaggle-Dataset)

- Moderate size
- Meaningful features
- Binary classification

```{r}
load('data/hr.rda')
dim(train_hr)
```

Training
========================================================

Prepare data

- No need to normalize

```{r}
ind <- sample(nrow(train_hr))
train_ind <- ind[1:10000]
test_ind <- ind[10001:14999]

x <- train_hr[train_ind, -1]
# zero-based class label
y <- train_hr[train_ind, 1]

x.test <- train_hr[test_ind, -1]
y.test <- train_hr[test_ind, 1]
```

Training
========================================================

Cross Validation

![](./img/cv.png)

Training
========================================================

Cross Validation

```{r}
param <- list("objective" = "binary:logistic",
              "eval_metric" = "auc")

bst.cv <- xgb.cv(param = param, data = x, label = y, 
                 nfold = 3, nrounds = 10)
```

Training
========================================================

Cross Validation

```{r}
bst.cv
```

Training
========================================================

- Num of trees: `nrounds`, `eta`
- Depth of trees: `max_depth`, `min_child_weight`
- Randomness: `subsample`, `colsample_bytree`
- Penalty: `gamma`, `lambda`

Training
========================================================

Cross Validation

```{r}
param <- list("objective" = "binary:logistic",
              "eval_metric" = "auc",
              "max_depth" = 2, eta = 0.5)

bst.cv <- xgb.cv(param = param, data = x, label = y, 
                 nfold = 3, nrounds = 10)
```

Practice
========================================================

Play with parameters to see how the results change

Training
========================================================

```{r}
param <- list("objective" = "binary:logistic",
              "eval_metric" = "auc")
model <- xgboost(param = param, data = x, label = y, nrounds = 10)
```

Training
========================================================

predict

```{r}
pred <- predict(model, x.test)
length(pred)

require(AUC)
auc(roc(pred, as.factor(y.test)))
```

Practice
========================================================

Compare cross validation and test set.

Interpretation
========================================================

Feature importance

```{r}
importance <- xgb.importance(model = model)
importance
```

Interpretation
========================================================

Feature importance

```{r}
xgb.plot.importance(importance)
```

Practice
========================================================

How parameters change the importance

Interpretation
========================================================

visualize a tree

```{r, eval=FALSE}
xgb.plot.tree(feature_names = colnames(x),
              model = model, trees = 0)
```

Interpretation
========================================================

Information from the tree

- Cover
  - Sum of second order gradient
- Gain
  - improvement from this split

Interpretation
========================================================

*There are too many trees!*

```{r, eval=FALSE}
xgb.plot.multi.trees(model)
```

Interpretation
========================================================

*Even a single tree is too large!*

```{r}
xgb.plot.deepness(model)
```

Practice
========================================================

How parameters change the depth

Parameter Tuning
========================================================

A new dataset: [Otto Group Product Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge)

- Moderate size
- Anonymous features
- Multi-classification

```{r}
load('data/otto.rda')
dim(train_otto)
```

Parameter Tuning
========================================================

```{r}
x <- train_otto[, -1]
# zero-based class label
y <- train_otto[, 1] - 1

param <- list("objective" = "multi:softprob",
              "eval_metric" = "mlogloss",
              "num_class" = 9)
```

Parameter Tuning
========================================================

```{r}
bst.cv <- xgb.cv(param = param, data = x, label = y, 
                 nfold = 3, nrounds = 5)
```

Parameter Tuning
========================================================

What are tunable parameters?

Function doc:

```{r, eval=FALSE}
?xgb.train
```

Online doc:

- http://xgboost.readthedocs.io/en/latest/parameter.html#parameters-in-r-package

Parameter Tuning
========================================================

where to look at first

- Objective
- Metric
- eta/nrounds

Parameter Tuning
========================================================

Overfitting

- shallower trees: `max_depth`, `min_child_weight`
- stronger randomness: `subsample`, `colsample_bytree`
- stronger penalty: `gamma`, `lambda`
- domain knowledge: `monotone_constraints`
  
Parameter Tuning
========================================================

Underfitting

- deeper trees: `max_depth`, `min_child_weight`
- weaker randomness:  `subsample`, `colsample_bytree`
- weaker penalty: `gamma`, `lambda`
- parallel trees: `num_parallel_tree`

Practice
========================================================

Tune your parameters and hit mlogloss 0.5!

Parameter Tuning
========================================================

cross validation

- The silver bullet?
  - Imbalanced class
  - Time-sensitive data

Parameter Tuning
========================================================

- trial-and-error
- grid search
- automatic tuning

Go even Faster
========================================================

Histogram

```{r, eval=FALSE}
params <- list(tree_method = 'hist')
```

Go even Faster
========================================================

Depth-wise  v.s. Loss-wise

- depth-wise
```{r, eval=FALSE}
params <- list(grow_policy = 'depthwise')
```
- loss-wise
```{r, eval=FALSE}
params <- list(grow_policy = 'lossguide')
```

Practice
========================================================

Use histogram to speed training up

About
========================================================

- github: https://github.com/dmlc/xgboost
- forum: https://discuss.xgboost.ai
- doc: https://xgboost.readthedocs.io/en/latest

Q&A
========================================================

